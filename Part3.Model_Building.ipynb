{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP 3 Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Qi Gui\n",
    "##### Yichen Li\n",
    "##### Junzhe Yin\n",
    "##### Jiatian Ye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART3. ML Models Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"group3_project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-72-105.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>group3_project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f98d0656350>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-72-105.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>group3_project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=group3_project>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer,IndexToString, VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data in parquet format\n",
    "mydf = spark.read.parquet('s3://bigdata-group3/mergedata/hardrive.parquet/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns with large number of NA\n",
    "drop_columns=[\n",
    "    'smart_2_normalized',\n",
    "    'smart_2_raw',\n",
    "    'smart_8_normalized',\n",
    "    'smart_8_raw',\n",
    "    'smart_11_normalized',\n",
    "    'smart_11_raw',\n",
    "    'smart_13_normalized',\n",
    "    'smart_13_raw',\n",
    "    'smart_15_normalized',\n",
    "    'smart_15_raw',\n",
    "    'smart_22_normalized',\n",
    "    'smart_22_raw',\n",
    "    'smart_177_normalized',\n",
    "    'smart_177_raw',\n",
    "    'smart_179_normalized',\n",
    "    'smart_179_raw',\n",
    "    'smart_181_normalized',\n",
    "    'smart_181_raw',\n",
    "    'smart_182_normalized',\n",
    "    'smart_182_raw',\n",
    "    'smart_183_normalized',\n",
    "    'smart_183_raw',\n",
    "    'smart_184_normalized',\n",
    "    'smart_184_raw',\n",
    "    'smart_189_normalized',\n",
    "    'smart_189_raw',\n",
    "    'smart_191_normalized',\n",
    "    'smart_191_raw',\n",
    "    'smart_195_normalized',\n",
    "    'smart_195_raw',\n",
    "    'smart_196_normalized',\n",
    "    'smart_196_raw',\n",
    "    'smart_200_normalized',\n",
    "    'smart_200_raw',\n",
    "    'smart_201_normalized',\n",
    "    'smart_201_raw',\n",
    "    'smart_220_normalized',\n",
    "    'smart_220_raw',\n",
    "    'smart_222_normalized',\n",
    "    'smart_222_raw',\n",
    "    'smart_223_normalized',\n",
    "    'smart_223_raw',\n",
    "    'smart_224_normalized',\n",
    "    'smart_224_raw',\n",
    "    'smart_225_normalized',\n",
    "    'smart_225_raw',\n",
    "    'smart_226_normalized',\n",
    "    'smart_226_raw',\n",
    "    'smart_235_normalized',\n",
    "    'smart_235_raw',\n",
    "    'smart_250_normalized',\n",
    "    'smart_250_raw',\n",
    "    'smart_251_normalized',\n",
    "    'smart_251_raw',\n",
    "    'smart_252_normalized',\n",
    "    'smart_252_raw',\n",
    "    'smart_254_normalized',\n",
    "    'smart_254_raw',   \n",
    "    'smart_255_normalized',\n",
    "    'smart_255_raw'\n",
    "]\n",
    "mydf_new=mydf.drop(*drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'serial_number',\n",
       " 'model',\n",
       " 'capacity_bytes',\n",
       " 'failure',\n",
       " 'smart_1_normalized',\n",
       " 'smart_1_raw',\n",
       " 'smart_3_normalized',\n",
       " 'smart_3_raw',\n",
       " 'smart_4_normalized',\n",
       " 'smart_4_raw',\n",
       " 'smart_5_normalized',\n",
       " 'smart_5_raw',\n",
       " 'smart_7_normalized',\n",
       " 'smart_7_raw',\n",
       " 'smart_9_normalized',\n",
       " 'smart_9_raw',\n",
       " 'smart_10_normalized',\n",
       " 'smart_10_raw',\n",
       " 'smart_12_normalized',\n",
       " 'smart_12_raw',\n",
       " 'smart_187_normalized',\n",
       " 'smart_187_raw',\n",
       " 'smart_188_normalized',\n",
       " 'smart_188_raw',\n",
       " 'smart_190_normalized',\n",
       " 'smart_190_raw',\n",
       " 'smart_192_normalized',\n",
       " 'smart_192_raw',\n",
       " 'smart_193_normalized',\n",
       " 'smart_193_raw',\n",
       " 'smart_194_normalized',\n",
       " 'smart_194_raw',\n",
       " 'smart_197_normalized',\n",
       " 'smart_197_raw',\n",
       " 'smart_198_normalized',\n",
       " 'smart_198_raw',\n",
       " 'smart_199_normalized',\n",
       " 'smart_199_raw',\n",
       " 'smart_240_normalized',\n",
       " 'smart_240_raw',\n",
       " 'smart_241_normalized',\n",
       " 'smart_241_raw',\n",
       " 'smart_242_normalized',\n",
       " 'smart_242_raw']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the exploratory analysis of our columns, we decide to build models to evaluate their classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, IndexToString, VectorAssembler, Binarizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with null value\n",
    "mydf_new=mydf_new.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 57364274 rows after dropping null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57364274"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf_new.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not include \"date\",\"model\",\"serial_number\" columns in our data since they are string value and have no meaning explaining the failure.Morover, we remove the failure from the \"features\" since it is the dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname=mydf_new.columns\n",
    "colname.remove('date')\n",
    "colname.remove('failure')\n",
    "colname.remove('model')\n",
    "colname.remove('serial_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aseember the independent variables as \"features\"\n",
    "vectorAssembler_features = VectorAssembler(\n",
    "    inputCols=colname, \n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split our data into traininig and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 45892843\n",
      "Number of testing records : 10324465\n",
      "Number of prediction records : 1146966\n"
     ]
    }
   ],
   "source": [
    "splitted_data = mydf_new.randomSplit([0.8, 0.18, 0.02], 24)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "predict_data = splitted_data[2]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))\n",
    "print(\"Number of prediction records : \" + str(predict_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the failure as dependent variable and others as independent variabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(labelCol=\"failure\", featuresCol=\"features\",maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log = Pipeline(stages=[vectorAssembler_features, log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log = pipeline_log.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy = 0.853669\n"
     ]
    }
   ],
   "source": [
    "predictions_log = model_log.transform(test_data)\n",
    "evaluatorLog = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\").setLabelCol(\"failure\")\n",
    "accuracy_log = evaluatorLog.evaluate(predictions_log)\n",
    "print(\"Logistic Regression Accuracy = %g\" % accuracy_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic model has 85.37% accuracy predicting the failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try ramdom forest to classify and predict the failure column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"failure\", featuresCol=\"features\", numTrees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain features and rf in a pipeline\n",
    "pipeline_rf= Pipeline(stages=[vectorAssembler_features, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.6 ms, sys: 23.5 ms, total: 82 ms\n",
      "Wall time: 9min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_rf = pipeline_rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy = 0.790199\n"
     ]
    }
   ],
   "source": [
    "predictions_rf = model_rf.transform(test_data)\n",
    "evaluatorRf = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\").setLabelCol(\"failure\")\n",
    "accuracy_rf = evaluatorRf.evaluate(predictions_rf)\n",
    "\n",
    "print(\"Random Forest Accuracy = %g\" % accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest model has 79.02% accuracy predicting the failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. GBTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still we use the failure as label column and other numeric columns as feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=5, maxDepth=2, featuresCol=\"features\",labelCol=\"failure\", seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the pipeline\n",
    "pipeline_gbt= Pipeline(stages=[vectorAssembler_features, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbt = pipeline_gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Tree Accuracy = 0.74321\n"
     ]
    }
   ],
   "source": [
    "predictions_gbt = model_gbt.transform(test_data)\n",
    "evaluatorGbt = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\").setLabelCol(\"failure\")\n",
    "accuracy_gbt = evaluatorGbt.evaluate(predictions_gbt)\n",
    "\n",
    "print(\"Gradient Boosting Tree Accuracy = %g\" % accuracy_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Tree has 74.321% accuracy predicting the failure.\n",
    "Among the three classification models we built, logistic regression performs best predicting the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
